{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 2023 COMP 4446 / 5046 Assignment 1\n",
        "\n",
        "Assingment 1 is an **individual** assessment. Please note the University's [Academic dishonesty and plagiarism policy](https://www.sydney.edu.au/students/academic-dishonesty.html).\n",
        "\n",
        "Submission Deadline: Friday, March 17th, 2023, 11:59pm\n",
        "\n",
        "Submit via Canvas:\n",
        "- Your notebook\n",
        "- Run all cells before saving the notebook, so we can see your output\n",
        "\n",
        "In this assignment, we will explore ways to predict the length of a Wikipedia article based on the first 100 tokens in the article. Such a model could be used to explore whether there are systematic biases in the types of articles that get more detail.\n",
        "\n",
        "If you are working in another language, please make sure to clearly indicate which part of your code is running which section of the assignment and produce output that provides all necessary information. Submit your code, example outputs, and instructions for executing it.\n",
        "\n",
        "Note: This assignment contains topics that are not covered at the time of release. Each section has information about which lectures and/or labs covered the relevant material. We are releasing it now so you can (1) start working on some parts early, and (2) know what will be in the assignment when you attend the relevant labs and lectures."
      ],
      "metadata": {
        "id": "UOKBV2uWZ9U_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TODO: Copy and Name this File**\n",
        "Make a copy of this notebook in your own Google Drive (File -> Save a Copy in Drive) and change the filename, replacing `YOUR-UNIKEY`. For example, for a person with unikey `mcol1997`, the filename should be:\n",
        "\n",
        "`COMP-4446-5046_Assignment1_mcol1997.ipynb`"
      ],
      "metadata": {
        "id": "FA3m7neId4ow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Readme\n",
        "*If there is something you want to tell the marker about your submission, please mention it here.* "
      ],
      "metadata": {
        "id": "qut4Xg5mbYXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "In this assignment for the pre-processing of the training text for section 1-2 i just split it based on the space. In total i generated 7739 tokens from the training text after splitting it based on the filter of occurring atleast 10 times. In this 10 is included in the calculation. \n",
        "For section 3 the entire training text was used after basic regex pre-processing.\n"
      ],
      "metadata": {
        "id": "YaX3ihzU7uDL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Download [DO NOT MODIFY THIS]\n",
        "\n",
        "We have already constructed a dataset for you using a recent dump of data from Wikipedia. Both the training and test datasets are provided in the form of csv files (training_data.csv, test_data.csv) and can be downloaded from Google Drive using the code below. Each row of the data contains:\n",
        "\n",
        "- The length of the article\n",
        "- The title of the article\n",
        "- The first 100 tokens of the article\n",
        "\n",
        "In case you are curious, we constructed this dataset as follows:\n",
        "1. Downloaded [a recent dump](https://dumps.wikimedia.org/) of English wikipedia.\n",
        "2. Ran [WikiExtractor](https://github.com/attardi/wikiextractor) to get the contents of the pages.\n",
        "3. Filtered out very short pages.\n",
        "4. Ran [SpaCy](https://spacy.io/) with the `en_core_web_lg` model to tokenise the pages (Note, SpaCy's development is led by an alumnus of USyd!).\n",
        "5. Counted the tokens and saved the relevant data in the format described above.\n",
        "\n",
        "This code will download the data. **DO NOT MODIFY IT**"
      ],
      "metadata": {
        "id": "-Ib68RAoatjk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94u_ipMMZ6Cu",
        "outputId": "606eb052-635c-4d0b-86b8-05f8ef7b73b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------\n",
            "Size of training data: 9859\n",
            "Size of development data: 994\n",
            "Size of test data: 991\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Sample Data\n",
            "LABEL: 6453 / SENTENCE: ['Anarchism', 'Anarchism is a political philosophy and movement that is skeptical of all justifications for authority and seeks to abolish the institutions it claims maintain unnecessary coercion and hierarchy , typically including , though not necessarily limited to , governments , nation states , and capitalism . Anarchism advocates for the replacement of the state with stateless societies or other forms of free associations . As a historically left - wing movement , usually placed on the farthest left of the political spectrum , it is usually described alongside communalism and libertarian Marxism as the libertarian wing ( libertarian socialism )']\n",
            "------------------------------------\n",
            "6453\n",
            "Anarchism\n",
            "Anarchism is a political philosophy and movement that is skeptical of all justifications for authori...\n",
            "\n",
            "3528\n",
            "Albedo\n",
            "Albedo (; ) is the measure of the diffuse reflection of solar radiation out of the total solar radia...\n",
            "\n",
            "1265\n",
            "A\n",
            "A , or a , is the first letter and the first vowel of the Latin alphabet , used in the modern Englis...\n",
            "\n",
            "11591\n",
            "Alabama\n",
            "Alabama ( ) is a state in the Southeastern region of the United States , bordered by Tennessee to th...\n",
            "\n",
            "5865\n",
            "Achilles\n",
            "In Greek mythology , Achilles ( ) or Achilleus ( ) was a hero of the Trojan War , the greatest of al...\n",
            "\n",
            "13412\n",
            "Abraham Lincoln\n",
            "Abraham Lincoln ( ; February 12 , 1809   – April 15 , 1865 ) was an American lawyer , politician , a...\n",
            "\n",
            "9485\n",
            "Aristotle\n",
            "Aristotle (; \" Aristotélēs \" , ; 384–322   BC ) was a Greek philosopher and polymath during the Clas...\n",
            "\n",
            "1683\n",
            "An American in Paris\n",
            "An American in Paris is a jazz - influenced orchestral piece by American composer George Gershwin fi...\n",
            "\n",
            "149\n",
            "Academy Award for Best Production Design\n",
            "The Academy Award for Best Production Design recognizes achievement for art direction in film . The ...\n",
            "\n",
            "7178\n",
            "Academy Awards\n",
            "The Academy Awards , better known as the Oscars , are awards for artistic and technical merit for th...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## DO NOT MODIFY THIS CODE\n",
        "# Code to download files into Colaboratory\n",
        "\n",
        "# Install the PyDrive library\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "# Import libraries for accessing Google Drive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Function to read the file, save it on the machine this colab is running on, and then read it in\n",
        "import csv\n",
        "def read_file(file_id, filename):\n",
        "  downloaded = drive.CreateFile({'id':file_id})\n",
        "  downloaded.GetContentFile(filename)\n",
        "  with open(filename) as src:\n",
        "    reader = csv.reader(src)\n",
        "    data = [r for r in reader]\n",
        "  return data\n",
        "\n",
        "# Calls to get the data\n",
        "# If you need to access the data directly (e.g., you are running experiments on a local machine), use these links:\n",
        "# - Training, https://drive.google.com/file/d/1-UGFS8D-qglAX-czU38KaM4jQVCoNe0W/view?usp=share_link\n",
        "# - Dev, https://drive.google.com/file/d/1RWMEf0mdJMTkWc7dvN0ioks8bjujqZaN/view?usp=share_link\n",
        "# - Test, https://drive.google.com/file/d/1YVPNzdIFSMmVPeLBP-gf5DOIed3oRFyB/view?usp=share_link\n",
        "training_data = read_file('1-UGFS8D-qglAX-czU38KaM4jQVCoNe0W', \"/content/training_data.csv\")\n",
        "dev_data = read_file('1RWMEf0mdJMTkWc7dvN0ioks8bjujqZaN', \"/content/dev_data.csv\")\n",
        "test_data = read_file('1YVPNzdIFSMmVPeLBP-gf5DOIed3oRFyB', \"/content/test_data.csv\")\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Size of training data: {0}\".format(len(training_data)))\n",
        "print(\"Size of development data: {0}\".format(len(dev_data)))\n",
        "print(\"Size of test data: {0}\".format(len(test_data)))\n",
        "print(\"------------------------------------\")\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Sample Data\")\n",
        "print(\"LABEL: {0} / SENTENCE: {1}\".format(training_data[0][0], training_data[0][1:]))\n",
        "print(\"------------------------------------\")\n",
        "\n",
        "# Preview of the data in the csv file, which has three columns: \n",
        "# (1) length of article, (2) title of the article, (3) first 100 words in the article\n",
        "for v in training_data[:10]:\n",
        "  print(\"{}\\n{}\\n{}\\n\".format(v[0], v[1], v[2][:100] + \"...\"))\n",
        "\n",
        "# Store the data in lists and mofidy the length value to be in [0, 1]\n",
        "training_lengths = [min(1.0, int(r[0])/10000) for r in training_data]\n",
        "training_text = [r[2] for r in training_data]\n",
        "\n",
        "dev_lengths = [min(1.0, int(r[0])/10000) for r in dev_data]\n",
        "dev_text = [r[2] for r in dev_data]\n",
        "\n",
        "test_lengths = [min(1.0, int(r[0])/10000) for r in test_data]\n",
        "test_text = [r[2] for r in test_data]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Predicting article length from initial content\n",
        "\n",
        "This section relates to content from **the week 1 lecture and the week 2 lab**.\n",
        "\n",
        "In this section, you will implement training and evaluation of a linear model (as seen in the week 2 lab) to predict the length of a wikipedia article from its first 100 words. You will represent the text using a Bag of Words model (as seen in the week 1 lecture)."
      ],
      "metadata": {
        "id": "QwiKfKQtphIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Word Mapping [2pt]\n",
        "\n",
        "In the code block below, write code to go through the training data and for any word that occurs at least 10 times:\n",
        "- Assign it a unique ID (consecutive, starting at 0)\n",
        "- Place it in a dictionary that maps from the word to the ID"
      ],
      "metadata": {
        "id": "DAGSol9qHIj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "# First splitting the sentences into words. Then we go through the list of words and if it exists in the dictionary then we add one to its countm, otherwise it equals to 1 \n",
        "#Then we go through the dictionary and store those counts exceeding 10 in another dictionary and then return it.\n",
        "def word_mapping(training_data):\n",
        "  new_dict = {}\n",
        "  for elem in training_data:\n",
        "    words = elem.split(\" \")\n",
        "    for elem1 in words :\n",
        "      if elem1 in new_dict :\n",
        "        new_dict[elem1] += 1\n",
        "      else : \n",
        "        new_dict[elem1] = 1\n",
        "  filtered_dict = {}\n",
        "  for key, value in new_dict.items():\n",
        "    if value >= 10:\n",
        "      filtered_dict[key] = value\n",
        "  final_dict = {}\n",
        "  for k, number in enumerate(filtered_dict):\n",
        "    final_dict[number] = k\n",
        "  return final_dict\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nsNf7pa5a9J6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Data to Bag-of-Words Tensors [2pt]\n",
        "\n",
        "In the code block below, write code to prepare the data in PyTorch tensors.\n",
        "\n",
        "The text should be converted to a bag of words (ie., a vector the length of the vocabulary in the mapping in the previous step, with counts of the words in the text)."
      ],
      "metadata": {
        "id": "8r3Ej4fBIKJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from collections import Counter\n",
        "\n",
        "def bow_converter(t_text, tokens):\n",
        "  bow_tensor = torch.zeros(len(t_text), len(tokens), dtype=torch.float32)\n",
        "  for i in range(len(t_text)):\n",
        "    words = t_text[i].split(\" \")\n",
        "    word_counts = Counter(words)\n",
        "    for word, count in word_counts.items():\n",
        "      if word in tokens:\n",
        "        bow_tensor[i, tokens[word]] = count\n",
        "  return bow_tensor\n",
        "\n",
        "x_data = bow_converter(training_text, word_mapping(training_text))\n",
        "y_data = torch.Tensor(training_lengths)\n",
        "\n",
        "#We use the same vocabulary from the training_text so that the matrices multiply.\n",
        "x_dev_data = bow_converter(dev_text, word_mapping(training_text))\n",
        "y_dev_data = torch.Tensor(dev_lengths)\n",
        "\n",
        "x_test_data = bow_converter(test_text, word_mapping(training_text))\n",
        "y_test_data = torch.Tensor(test_lengths)"
      ],
      "metadata": {
        "id": "7Or-645qIKQC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Model Creation [2pt]\n",
        "\n",
        "Construct a linear model with an SGD optimiser (we recommend a learning rate of `1e-4`) and mean squared error as the loss."
      ],
      "metadata": {
        "id": "nsb--KW_I_F7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Mean square error function.\n",
        "def mse(x1, x2):\n",
        "  diff = x1 - x2\n",
        "  return torch.sum(diff*diff)/diff.numel()\n",
        "\n",
        "linearRegression =  nn.Linear(7739,1)\n",
        "optimizer = torch.optim.SGD(linearRegression.parameters(), lr=1e-4)\n",
        "\n",
        "# Define the loss function\n",
        "loss_func = F.mse_loss\n",
        "\n",
        "# Calculate loss\n",
        "loss = loss_func(linearRegression(x_data), y_data.unsqueeze(1))\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "JEzbIGe4I_QK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d9315bc-359f-4689-b4d3-247b8b592221"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2853, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Training [2pt]\n",
        "\n",
        "Write a loop to train your model for 100 epochs, printing performance on the dev set every 10 epochs."
      ],
      "metadata": {
        "id": "AjHulTA6JQ3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_of_epochs = 100\n",
        "\n",
        "display_interval = 10\n",
        "\n",
        "for epoch in range(no_of_epochs):\n",
        "\n",
        "  predictions = linearRegression(x_data)\n",
        "  loss = loss_func(predictions, y_data.unsqueeze(1))\n",
        "  loss.backward()\n",
        "  training_loss = mse(linearRegression(x_data), y_data.unsqueeze(1))   \n",
        "  print(\"Optimised:\", \"training loss=\", \"{:.9f}\".format(training_loss.data))\n",
        "  optimizer.step() #call step() to automatically update the parameters through our defined optimizer, which can be called once after backward()\n",
        "  optimizer.zero_grad() #reset the gradient as what we did before\n",
        "  if epoch % display_interval == 0 :\n",
        "      \n",
        "      # calculate the loss of the current model\n",
        "      predictions1 = linearRegression(x_dev_data)\n",
        "      loss = loss_func(predictions1, y_dev_data.unsqueeze(1))          \n",
        "      print(\"Epoch:\", '%04d' % (epoch), \"dev loss=\", \"{:.8f}\".format(loss))\n"
      ],
      "metadata": {
        "id": "WZdODnGdJQ8w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "537653ba-1286-4503-98fb-668999cd53f1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimised: training loss= 0.285268039\n",
            "Epoch: 0000 dev loss= 0.28043595\n",
            "Optimised: training loss= 0.275471807\n",
            "Optimised: training loss= 0.266153187\n",
            "Optimised: training loss= 0.257288843\n",
            "Optimised: training loss= 0.248856589\n",
            "Optimised: training loss= 0.240835428\n",
            "Optimised: training loss= 0.233205289\n",
            "Optimised: training loss= 0.225947052\n",
            "Optimised: training loss= 0.219042584\n",
            "Optimised: training loss= 0.212474629\n",
            "Optimised: training loss= 0.206226766\n",
            "Epoch: 0010 dev loss= 0.20529249\n",
            "Optimised: training loss= 0.200283363\n",
            "Optimised: training loss= 0.194629580\n",
            "Optimised: training loss= 0.189251289\n",
            "Optimised: training loss= 0.184135064\n",
            "Optimised: training loss= 0.179268092\n",
            "Optimised: training loss= 0.174638227\n",
            "Optimised: training loss= 0.170233890\n",
            "Optimised: training loss= 0.166044101\n",
            "Optimised: training loss= 0.162058368\n",
            "Optimised: training loss= 0.158266753\n",
            "Epoch: 0020 dev loss= 0.15936065\n",
            "Optimised: training loss= 0.154659748\n",
            "Optimised: training loss= 0.151228383\n",
            "Optimised: training loss= 0.147964120\n",
            "Optimised: training loss= 0.144858733\n",
            "Optimised: training loss= 0.141904488\n",
            "Optimised: training loss= 0.139094055\n",
            "Optimised: training loss= 0.136420384\n",
            "Optimised: training loss= 0.133876801\n",
            "Optimised: training loss= 0.131456971\n",
            "Optimised: training loss= 0.129154831\n",
            "Epoch: 0030 dev loss= 0.13121721\n",
            "Optimised: training loss= 0.126964703\n",
            "Optimised: training loss= 0.124881051\n",
            "Optimised: training loss= 0.122898705\n",
            "Optimised: training loss= 0.121012747\n",
            "Optimised: training loss= 0.119218446\n",
            "Optimised: training loss= 0.117511339\n",
            "Optimised: training loss= 0.115887180\n",
            "Optimised: training loss= 0.114341892\n",
            "Optimised: training loss= 0.112871677\n",
            "Optimised: training loss= 0.111472815\n",
            "Epoch: 0040 dev loss= 0.11391870\n",
            "Optimised: training loss= 0.110141881\n",
            "Optimised: training loss= 0.108875528\n",
            "Optimised: training loss= 0.107670613\n",
            "Optimised: training loss= 0.106524132\n",
            "Optimised: training loss= 0.105433255\n",
            "Optimised: training loss= 0.104395241\n",
            "Optimised: training loss= 0.103407525\n",
            "Optimised: training loss= 0.102467671\n",
            "Optimised: training loss= 0.101573318\n",
            "Optimised: training loss= 0.100722246\n",
            "Epoch: 0050 dev loss= 0.10324175\n",
            "Optimised: training loss= 0.099912383\n",
            "Optimised: training loss= 0.099141672\n",
            "Optimised: training loss= 0.098408215\n",
            "Optimised: training loss= 0.097710215\n",
            "Optimised: training loss= 0.097045921\n",
            "Optimised: training loss= 0.096413702\n",
            "Optimised: training loss= 0.095811971\n",
            "Optimised: training loss= 0.095239282\n",
            "Optimised: training loss= 0.094694167\n",
            "Optimised: training loss= 0.094175324\n",
            "Epoch: 0060 dev loss= 0.09661537\n",
            "Optimised: training loss= 0.093681455\n",
            "Optimised: training loss= 0.093211345\n",
            "Optimised: training loss= 0.092763826\n",
            "Optimised: training loss= 0.092337802\n",
            "Optimised: training loss= 0.091932222\n",
            "Optimised: training loss= 0.091546103\n",
            "Optimised: training loss= 0.091178484\n",
            "Optimised: training loss= 0.090828463\n",
            "Optimised: training loss= 0.090495184\n",
            "Optimised: training loss= 0.090177834\n",
            "Epoch: 0070 dev loss= 0.09247275\n",
            "Optimised: training loss= 0.089875616\n",
            "Optimised: training loss= 0.089587837\n",
            "Optimised: training loss= 0.089313745\n",
            "Optimised: training loss= 0.089052685\n",
            "Optimised: training loss= 0.088804036\n",
            "Optimised: training loss= 0.088567190\n",
            "Optimised: training loss= 0.088341564\n",
            "Optimised: training loss= 0.088126615\n",
            "Optimised: training loss= 0.087921806\n",
            "Optimised: training loss= 0.087726675\n",
            "Epoch: 0080 dev loss= 0.08985770\n",
            "Optimised: training loss= 0.087540723\n",
            "Optimised: training loss= 0.087363519\n",
            "Optimised: training loss= 0.087194629\n",
            "Optimised: training loss= 0.087033659\n",
            "Optimised: training loss= 0.086880200\n",
            "Optimised: training loss= 0.086733907\n",
            "Optimised: training loss= 0.086594418\n",
            "Optimised: training loss= 0.086461410\n",
            "Optimised: training loss= 0.086334556\n",
            "Optimised: training loss= 0.086213566\n",
            "Epoch: 0090 dev loss= 0.08818577\n",
            "Optimised: training loss= 0.086098149\n",
            "Optimised: training loss= 0.085988045\n",
            "Optimised: training loss= 0.085882992\n",
            "Optimised: training loss= 0.085782729\n",
            "Optimised: training loss= 0.085687041\n",
            "Optimised: training loss= 0.085595682\n",
            "Optimised: training loss= 0.085508466\n",
            "Optimised: training loss= 0.085425183\n",
            "Optimised: training loss= 0.085345618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Measure Accuracy [2pt]\n",
        "\n",
        "In the code block below, write code to evaluate your model on the test set."
      ],
      "metadata": {
        "id": "bCwG22mOoyJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#First converting the test data into bag of words tensor and the output variable to a tensor.\n",
        "x_test_data = bow_converter(test_text, word_mapping(training_text))\n",
        "y_test_data = torch.Tensor(test_lengths)\n",
        "\n",
        "testing_loss = loss_func(linearRegression(x_test_data), y_test_data.unsqueeze(1)) \n",
        "print(\"Testing loss=\", \"{:.9f}\".format(testing_loss.data))\n",
        "print(\"Absolute mean square loss difference:\", \"{:.9f}\".format(abs(\n",
        "      training_loss.data - testing_loss.data)))\n",
        "                \n"
      ],
      "metadata": {
        "id": "gs_yX-Gnoydf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "171fb8f1-e459-424e-ac6d-0ab99445c9ab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing loss= 0.076088734\n",
            "Absolute mean square loss difference: 0.009256884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The absolute mean square loss difference signifies quite an accurate model."
      ],
      "metadata": {
        "id": "M6TddIeQfXpK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Analyse the Model [2pt]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In the code block below, write code to identify the 50 words with the highest weights and the 50 words with the lowest weights."
      ],
      "metadata": {
        "id": "1TE7CMqZoylt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "numpy_weights = linearRegression.weight.data.detach().numpy()\n",
        "weights = numpy_weights[0]\n",
        "#Retrieve the indexes of the top 50 and bottom 50  weights without re-organizing.\n",
        "top_weights = weights.argsort()[::-1][:50]\n",
        "lowest_weights = weights.argsort()[:50]\n",
        "training_words = list(word_mapping(training_text).keys())\n",
        "print(\"Top 50 words are :\")\n",
        "for i in top_weights:\n",
        "    print(training_words[i] + \":\" + str(weights[i]))\n",
        "print(\"\\n\")\n",
        "print(\"Bottom 50 words are: \")\n",
        "for i in lowest_weights:\n",
        "    print(training_words[i] + \":\" + str(weights[i]))"
      ],
      "metadata": {
        "id": "T4bmSbhhoy7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ff159c5-3ab8-4f6a-a569-13c1c9f7f787"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 50 words are :\n",
            ",:0.023540823\n",
            "–:0.011511847\n",
            "around:0.011445019\n",
            "border:0.011413005\n",
            "roughly:0.011399967\n",
            "rights:0.0113901235\n",
            "colonial:0.011382417\n",
            "monarch:0.011377085\n",
            "figure:0.011366423\n",
            "Telecom:0.0113570895\n",
            "complexity:0.011353206\n",
            "8th:0.01135245\n",
            "Malaysian:0.011342986\n",
            "twelfth:0.011336759\n",
            "behavioral:0.011331191\n",
            "paramilitary:0.01132567\n",
            "erotic:0.011325666\n",
            "Berber:0.0113214385\n",
            "applicable:0.011320333\n",
            "fiction:0.011318757\n",
            "theological:0.011314333\n",
            "write:0.011314122\n",
            "polymath:0.011312485\n",
            "not:0.01131248\n",
            "Azerbaijan:0.011311639\n",
            "Roads:0.011305569\n",
            "Easter:0.011302605\n",
            "Estonia:0.0112977885\n",
            " :0.011297341\n",
            "Jr.:0.011295268\n",
            "solution:0.011293906\n",
            "anthem:0.01129315\n",
            "systematic:0.011290461\n",
            "bases:0.011287019\n",
            "Virginia:0.011287\n",
            "retired:0.011286539\n",
            "lineup:0.011278584\n",
            "Format:0.011271461\n",
            "anthology:0.011270631\n",
            "Carbon:0.011266927\n",
            "gland:0.011262301\n",
            "genius:0.01126016\n",
            "lipids:0.011252144\n",
            "boat:0.011231619\n",
            "existed:0.011231435\n",
            "aquarium:0.011230847\n",
            "Eugene:0.011230672\n",
            "weapon:0.011230532\n",
            "synthesis:0.011223185\n",
            "Olympic:0.011217556\n",
            "\n",
            "\n",
            "Bottom 50 words are: \n",
            "Agreement:-0.011355891\n",
            "designations:-0.011354113\n",
            "merged:-0.0113503635\n",
            "socially:-0.011349703\n",
            "Fujiwara:-0.011343667\n",
            "explored:-0.011341574\n",
            "1992:-0.011332016\n",
            "Vietnamese:-0.011331424\n",
            "approximate:-0.011330416\n",
            "leaders:-0.01132941\n",
            "spanning:-0.011329219\n",
            "rotating:-0.0113289\n",
            "quantum:-0.0113197565\n",
            "polyhedron:-0.011318895\n",
            "advanced:-0.011318257\n",
            "1937:-0.0113175055\n",
            "peninsula:-0.011315506\n",
            "claims:-0.01131515\n",
            "am:-0.011308256\n",
            "insulin:-0.011304371\n",
            "distinguishes:-0.011303065\n",
            "seizures:-0.011302444\n",
            "terrain:-0.011296886\n",
            "helix:-0.011291014\n",
            "Age:-0.011289592\n",
            "carrying:-0.011283015\n",
            "mobile:-0.011278837\n",
            "Duchy:-0.011257294\n",
            "hired:-0.011255221\n",
            "seats:-0.011254939\n",
            "head:-0.011254791\n",
            "inputs:-0.011250638\n",
            "chemist:-0.011249981\n",
            "earth:-0.011247801\n",
            "humor:-0.011247278\n",
            "symbol:-0.011246082\n",
            "coil:-0.01124023\n",
            "4:-0.011240057\n",
            "origins:-0.011239828\n",
            "capita:-0.011236632\n",
            "accession:-0.011233428\n",
            "receptors:-0.011230488\n",
            "drug:-0.011230476\n",
            "encompassing:-0.011229574\n",
            "researchers:-0.011228691\n",
            "numeral:-0.011222437\n",
            "saints:-0.011221287\n",
            "politicians:-0.011219908\n",
            "Up:-0.011204566\n",
            "syntax:-0.011201885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Compare Data Storage Methods\n",
        "\n",
        "This section relates to content from **the week 1 lecture and the week 2 lab**.\n",
        "\n",
        "Implement a variant of the model with a sparse vector for your input bag of words (See https://pytorch.org/docs/stable/sparse.html for how to switch a vector to be sparse). Use the default sparse vector type (COO)."
      ],
      "metadata": {
        "id": "l5fxNtitbFck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "linearRegression_new =  nn.Linear(7739,1)\n",
        "optimizer = torch.optim.SGD(linearRegression_new.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "# Define the loss function\n",
        "loss_func = F.mse_loss\n",
        "\n",
        "# Calculate loss\n",
        "loss = loss_func(linearRegression_new(x_data), y_data.unsqueeze(1))\n",
        "\n",
        "x_data_sparse = x_data.to_sparse()\n",
        "x_dev_data_sparse = bow_converter(dev_text, word_mapping(training_text)).to_sparse()\n",
        "x_test_data_sparse = bow_converter(test_text, word_mapping(training_text)).to_sparse()\n",
        "\n",
        "no_of_epochs = 100\n",
        "display_interval = 10\n",
        "\n",
        "for epoch in range(no_of_epochs):\n",
        "  predictions1 = linearRegression_new(x_data_sparse)\n",
        "  loss = loss_func(predictions1, y_data.unsqueeze(1))\n",
        "  loss.backward()\n",
        "  training_loss = mse(linearRegression_new(x_data_sparse), y_data.unsqueeze(1))   \n",
        "  #print(\"Optimised:\", \"training loss=\", \"{:.9f}\".format(training_loss.data))\n",
        "  optimizer.step() #call step() to automatically update the parameters through our defined optimizer, which can be called once after backward()\n",
        "  optimizer.zero_grad() #reset the gradient as what we did before\n",
        "  if epoch % display_interval == 0 :\n",
        "      \n",
        "      # calculate the loss of the current model\n",
        "      \n",
        "      predictions2 = linearRegression_new(x_dev_data_sparse)\n",
        "      loss = loss_func(predictions2, y_dev_data.unsqueeze(1))          \n",
        "      print(\"Epoch:\", '%04d' % (epoch), \"dev loss=\", \"{:.8f}\".format(loss))\n",
        "  \n",
        "testing_loss = loss_func(linearRegression_new(x_test_data), y_test_data.unsqueeze(1)) \n",
        "print(\"Testing loss=\", \"{:.9f}\".format(testing_loss.data))\n",
        "print(\"Absolute mean square loss difference:\", \"{:.9f}\".format(abs(\n",
        "      training_loss.data - testing_loss.data)))"
      ],
      "metadata": {
        "id": "jc7LbuE6bQjW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "101ea430-a8eb-4f0e-9c7a-56c376988e6a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0000 dev loss= 0.21340832\n",
            "Epoch: 0010 dev loss= 0.16284272\n",
            "Epoch: 0020 dev loss= 0.13214502\n",
            "Epoch: 0030 dev loss= 0.11349096\n",
            "Epoch: 0040 dev loss= 0.10213718\n",
            "Epoch: 0050 dev loss= 0.09520841\n",
            "Epoch: 0060 dev loss= 0.09096180\n",
            "Epoch: 0070 dev loss= 0.08834108\n",
            "Epoch: 0080 dev loss= 0.08670612\n",
            "Epoch: 0090 dev loss= 0.08566905\n",
            "Testing loss= 0.075715683\n",
            "Absolute mean square loss difference: 0.007577106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Training and Test Speed [2pt]\n",
        "Compare the time it takes to train and test the new model with the time it takes to train and test the old model.\n",
        "\n",
        "You can time the execution of a line of code using `%time`.\n",
        "See [this guide](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/01.07-Timing-and-Profiling.ipynb#scrollTo=z1gyaC_PNZUB) for more on timing."
      ],
      "metadata": {
        "id": "HkAPEr91qBTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the Training time and testing time for the Old Model First which uses x_data and y_data.\n"
      ],
      "metadata": {
        "id": "LZQLDusj4O1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Old Model Training Time\n",
        "%%time\n",
        "\n",
        "no_of_epochs = 100\n",
        "display_interval = 10\n",
        "\n",
        "for epoch in range(no_of_epochs):\n",
        "\n",
        "  predictions = linearRegression(x_data)\n",
        "  loss = loss_func(predictions, y_data.unsqueeze(1))\n",
        "  loss.backward()\n",
        "  training_loss = mse(linearRegression(x_data), y_data.unsqueeze(1))   \n",
        "  #print(\"Optimised:\", \"training loss=\", \"{:.9f}\".format(training_loss.data))\n",
        "  optimizer.step() #call step() to automatically update the parameters through our defined optimizer, which can be called once after backward()\n",
        "  optimizer.zero_grad() #reset the gradient as what we did before"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTNvBJZXN7LO",
        "outputId": "71b2e9df-5a21-4bb3-f685-896edf27228c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 7.4 s, sys: 10.3 ms, total: 7.41 s\n",
            "Wall time: 7.44 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Old Model Testing Time\n",
        "%%time\n",
        "\n",
        "testing_loss = loss_func(linearRegression(x_test_data), y_test_data.unsqueeze(1)) \n",
        "print(\"Testing loss=\", \"{:.9f}\".format(testing_loss.data))\n",
        "print(\"Absolute mean square loss difference:\", \"{:.9f}\".format(abs(\n",
        "      training_loss.data - testing_loss.data)))\n",
        "                "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4jiMcuW4k-P",
        "outputId": "cee8e850-2a84-4af6-f3d2-d5008c601a55"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing loss= 0.076088734\n",
            "Absolute mean square loss difference: 0.009180911\n",
            "CPU times: user 4.68 ms, sys: 21 µs, total: 4.7 ms\n",
            "Wall time: 3.58 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "New Model Training and Testing Time using Sparse Vectors"
      ],
      "metadata": {
        "id": "lWfZM60648j3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#New Model Training Time\n",
        "%%time\n",
        "\n",
        "no_of_epochs = 100\n",
        "display_interval = 10\n",
        "\n",
        "for epoch in range(no_of_epochs):\n",
        "  predictions1 = linearRegression_new(x_data_sparse)\n",
        "  loss = loss_func(predictions1, y_data.unsqueeze(1))\n",
        "  loss.backward()\n",
        "  training_loss = mse(linearRegression_new(x_data_sparse), y_data.unsqueeze(1))   \n",
        "  #print(\"Optimised:\", \"training loss=\", \"{:.9f}\".format(training_loss.data))\n",
        "  optimizer.step() #call step() to automatically update the parameters through our defined optimizer, which can be called once after backward()\n",
        "  optimizer.zero_grad() #reset the gradient as what we did before\n"
      ],
      "metadata": {
        "id": "HnRzVlA9qBYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8353569-485b-44cf-9985-cfc88c7e686f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.58 s, sys: 13.3 ms, total: 3.59 s\n",
            "Wall time: 3.58 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#New Model Testing Time\n",
        "%%time\n",
        "\n",
        "\n",
        "testing_loss = loss_func(linearRegression_new(x_test_data_sparse), y_test_data.unsqueeze(1)) \n",
        "print(\"Testing loss=\", \"{:.9f}\".format(testing_loss.data))\n",
        "print(\"Absolute mean square loss difference:\", \"{:.9f}\".format(abs(\n",
        "      training_loss.data - testing_loss.data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrYKth0o5OfD",
        "outputId": "1a55379f-0580-4f1c-c011-66d2dea1ce6d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing loss= 0.074423753\n",
            "Absolute mean square loss difference: 0.006830901\n",
            "CPU times: user 3.11 ms, sys: 0 ns, total: 3.11 ms\n",
            "Wall time: 2.65 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see above, the training and testing time by using the sparse vector representation of the training data (new model) is lower than the training and testing time for the older model."
      ],
      "metadata": {
        "id": "bHnlqySguB9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Switch to Word Embeddings\n",
        "\n",
        "This section relates to content from **the week 2 lecture and the week 3 lab**.\n",
        "\n",
        "In this section, you will implement a model based on word2vec.\n",
        "\n",
        "1. Use word2vec to learn embeddings for the words in your data.\n",
        "2. Represent each input document as the average of the word vectors for the words it contains.\n",
        "3. Train a linear regression model."
      ],
      "metadata": {
        "id": "Whic_heibGEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First we create a tensor based on the word embeddings generated by Word2Vec.\n",
        "\n",
        "import pprint\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# For parsing our XML data\n",
        "from lxml import etree \n",
        "\n",
        "# For data processing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# For implementing the word2vec family of algorithms\n",
        "from gensim.models import Word2Vec\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "def exercise3 (t_text):\n",
        "  normalized_text = []\n",
        "  for string in t_text:\n",
        "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "    normalized_text.append(tokens)\n",
        "  \n",
        "  sentences=[]\n",
        "  sentences=[word_tokenize(sentence) for sentence in normalized_text]\n",
        "  model = Word2Vec(sentences, vector_size = 100 ,window=5, min_count=1, workers=4, sg=0)\n",
        "  vocab, vectors = model.wv.key_to_index, model.wv.vectors\n",
        "\n",
        "  doc_mean = []\n",
        "  for doc in sentences:\n",
        "    doc_mat = []\n",
        "    for word in doc :\n",
        "      if word in vocab:\n",
        "        doc_mat.append(model.wv[word])\n",
        "    if len(doc_mat) != 0:\n",
        "      doc_mean.append(np.mean(doc_mat, axis=0))\n",
        "    else:\n",
        "      doc_mean.append(np.zeros(100))\n",
        "\n",
        "   \n",
        "  x = torch.Tensor(doc_mean)\n",
        "  #x = x.mean(dim=1, keepdim=True)\n",
        "  return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nA-x3rwObQ6_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee129f4b-1239-4ba1-9c5e-15381b9a9235"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x = exercise3(training_text)\n",
        "#import torch.optim as optim\n",
        "# Define the linear regression model\n",
        "l_model = nn.Linear(100, 1)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.SGD(l_model.parameters(), lr=1e-4)\n",
        "\n",
        "# Set the number of epochs\n",
        "num_epochs = 100\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    y_pred = l_model(x)\n",
        "    loss = mse(y_pred, y_data)\n",
        "    print(\"Optimised:\", \"training loss=\", \"{:.9f}\".format(loss.data))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTKhapoo8J5t",
        "outputId": "e34387db-0007-4148-dcf4-a607d62b110b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-b1a0ac41abd3>:43: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  x = torch.Tensor(doc_mean)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimised: training loss= 0.451790839\n",
            "Optimised: training loss= 0.448914379\n",
            "Optimised: training loss= 0.446060508\n",
            "Optimised: training loss= 0.443229139\n",
            "Optimised: training loss= 0.440420061\n",
            "Optimised: training loss= 0.437633127\n",
            "Optimised: training loss= 0.434868097\n",
            "Optimised: training loss= 0.432124883\n",
            "Optimised: training loss= 0.429403275\n",
            "Optimised: training loss= 0.426703125\n",
            "Optimised: training loss= 0.424024224\n",
            "Optimised: training loss= 0.421366394\n",
            "Optimised: training loss= 0.418729603\n",
            "Optimised: training loss= 0.416113526\n",
            "Optimised: training loss= 0.413518012\n",
            "Optimised: training loss= 0.410943031\n",
            "Optimised: training loss= 0.408388227\n",
            "Optimised: training loss= 0.405853599\n",
            "Optimised: training loss= 0.403338939\n",
            "Optimised: training loss= 0.400844038\n",
            "Optimised: training loss= 0.398368895\n",
            "Optimised: training loss= 0.395913213\n",
            "Optimised: training loss= 0.393476814\n",
            "Optimised: training loss= 0.391059607\n",
            "Optimised: training loss= 0.388661504\n",
            "Optimised: training loss= 0.386282235\n",
            "Optimised: training loss= 0.383921713\n",
            "Optimised: training loss= 0.381579846\n",
            "Optimised: training loss= 0.379256308\n",
            "Optimised: training loss= 0.376951128\n",
            "Optimised: training loss= 0.374664158\n",
            "Optimised: training loss= 0.372395158\n",
            "Optimised: training loss= 0.370144039\n",
            "Optimised: training loss= 0.367910624\n",
            "Optimised: training loss= 0.365694821\n",
            "Optimised: training loss= 0.363496453\n",
            "Optimised: training loss= 0.361315370\n",
            "Optimised: training loss= 0.359151542\n",
            "Optimised: training loss= 0.357004762\n",
            "Optimised: training loss= 0.354874849\n",
            "Optimised: training loss= 0.352761716\n",
            "Optimised: training loss= 0.350665152\n",
            "Optimised: training loss= 0.348585218\n",
            "Optimised: training loss= 0.346521616\n",
            "Optimised: training loss= 0.344474286\n",
            "Optimised: training loss= 0.342443138\n",
            "Optimised: training loss= 0.340427876\n",
            "Optimised: training loss= 0.338428557\n",
            "Optimised: training loss= 0.336444974\n",
            "Optimised: training loss= 0.334477007\n",
            "Optimised: training loss= 0.332524538\n",
            "Optimised: training loss= 0.330587417\n",
            "Optimised: training loss= 0.328665555\n",
            "Optimised: training loss= 0.326758891\n",
            "Optimised: training loss= 0.324867159\n",
            "Optimised: training loss= 0.322990388\n",
            "Optimised: training loss= 0.321128428\n",
            "Optimised: training loss= 0.319281071\n",
            "Optimised: training loss= 0.317448288\n",
            "Optimised: training loss= 0.315629900\n",
            "Optimised: training loss= 0.313825905\n",
            "Optimised: training loss= 0.312036067\n",
            "Optimised: training loss= 0.310260355\n",
            "Optimised: training loss= 0.308498621\n",
            "Optimised: training loss= 0.306750715\n",
            "Optimised: training loss= 0.305016667\n",
            "Optimised: training loss= 0.303296208\n",
            "Optimised: training loss= 0.301589280\n",
            "Optimised: training loss= 0.299895823\n",
            "Optimised: training loss= 0.298215687\n",
            "Optimised: training loss= 0.296548843\n",
            "Optimised: training loss= 0.294895083\n",
            "Optimised: training loss= 0.293254346\n",
            "Optimised: training loss= 0.291626513\n",
            "Optimised: training loss= 0.290011555\n",
            "Optimised: training loss= 0.288409263\n",
            "Optimised: training loss= 0.286819607\n",
            "Optimised: training loss= 0.285242468\n",
            "Optimised: training loss= 0.283677757\n",
            "Optimised: training loss= 0.282125384\n",
            "Optimised: training loss= 0.280585170\n",
            "Optimised: training loss= 0.279057145\n",
            "Optimised: training loss= 0.277541161\n",
            "Optimised: training loss= 0.276037097\n",
            "Optimised: training loss= 0.274544924\n",
            "Optimised: training loss= 0.273064464\n",
            "Optimised: training loss= 0.271595627\n",
            "Optimised: training loss= 0.270138413\n",
            "Optimised: training loss= 0.268692613\n",
            "Optimised: training loss= 0.267258286\n",
            "Optimised: training loss= 0.265835166\n",
            "Optimised: training loss= 0.264423281\n",
            "Optimised: training loss= 0.263022572\n",
            "Optimised: training loss= 0.261632800\n",
            "Optimised: training loss= 0.260254025\n",
            "Optimised: training loss= 0.258886129\n",
            "Optimised: training loss= 0.257528961\n",
            "Optimised: training loss= 0.256182462\n",
            "Optimised: training loss= 0.254846603\n",
            "Optimised: training loss= 0.253521293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Accuracy [1pt]\n",
        "\n",
        "Calculate the accuracy of your model."
      ],
      "metadata": {
        "id": "Yj4ogp1Rq3Tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "\n",
        "x_test_new = exercise3(test_text)\n",
        "y_test_new = torch.Tensor(test_lengths)\n",
        "testing_loss = mse(l_model(x_test_new), y_test_new) \n",
        "print(\"Testing loss=\", \"{:.9f}\".format(testing_loss.data))\n",
        "print(\"Absolute mean square loss difference:\", \"{:.9f}\".format(abs(\n",
        "      training_loss.data - testing_loss.data)))"
      ],
      "metadata": {
        "id": "TWP19fZKq3Zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "354c07a4-0ee9-4e2b-f538-93be129a09df"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing loss= 0.181005016\n",
            "Absolute mean square loss difference: 0.099750362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Speed [1pt]\n",
        "\n",
        "Calcualte how long it takes your model to be evaluated."
      ],
      "metadata": {
        "id": "zVXaNbLlq3fJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "%%time \n",
        "\n",
        "x_test = exercise3(test_text)\n",
        "y_test = torch.Tensor(test_lengths)\n",
        "testing_loss = mse(l_model(x_test), y_test) \n",
        "print(\"Testing loss=\", \"{:.9f}\".format(testing_loss.data))\n",
        "print(\"Absolute mean square loss difference:\", \"{:.9f}\".format(abs(\n",
        "      training_loss.data - testing_loss.data)))"
      ],
      "metadata": {
        "id": "zp8q-nZOq3ko",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f45441ae-ed9b-4774-ccf9-332aab1df8cb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing loss= 0.184044257\n",
            "Absolute mean square loss difference: 0.102789603\n",
            "CPU times: user 1.54 s, sys: 14.6 ms, total: 1.56 s\n",
            "Wall time: 1.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - Open-Ended Improvement\n",
        "\n",
        "This section relates to content from **the week 1, 2, and 3 lectures and the week 1, 2, and 3 labs**.\n",
        "\n",
        "This section is an open-ended opportunity to find ways to make your model more accurate and/or faster (e.g., use WordNet to generalise words, try different word features, other optimisers, etc).\n",
        "\n",
        "We encourage you to try several ideas to provide scope for comparisons.\n",
        "\n",
        "If none of your ideas work you can still get full marks for this section. You just need to justify the ideas and discuss why they may not have improved performance.\n"
      ],
      "metadata": {
        "id": "QOT_5vmFbGy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Ideas and Motivation [1pt]\n",
        "\n",
        "In **this** box, describe your ideas and why you think they will improve accuracy and/or speed.\n",
        "\n",
        "The main aim of my improvement is increasing the accuracy of the model. I have done that by building on section 3 of this assignment in which We average the word vectors from the vocabulary. The changes I have implemented are: \n",
        "\n",
        "1. Removing Stopwords, remove punctuation - In this during tokenisation, I removed some basic words which may affect the accuracy of the model since they occur quite frequently such as 'and' 'a'. Along with Stop words, I removed punctuations as well since we are interested in the actual words themselves. \n",
        "\n",
        "2. Fast Text : FastText was used since it accounts for out-of-vocabulary words (OOV). Since we were given the development set and test set for evaluation purposes, it contains new data that negatively affects the accuracy of the model and hence FastText allows us to deal with this better.\n",
        "\n",
        "\n",
        "3. Increasing the vector size and epochs : In the previous sections, the epochs were limited to 100 and in this section the epochs has been set to 200. This  increases the training time but improves the accuracy.\n",
        "\n",
        "4. Another minor change implemented which may have increased the accuracy is by changing the model algorithm from bow to skip gram."
      ],
      "metadata": {
        "id": "Zryd7CmcrIjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the improvement let us take the example of the word2vec representation and try to improve the model."
      ],
      "metadata": {
        "id": "0FGy2GHy86N7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Implementation [2pt]\n",
        "\n",
        "Implement your ideas"
      ],
      "metadata": {
        "id": "jn1aesgGrJSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Text\n",
        "# Your code goes here\n",
        "from gensim.models import FastText\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "\n",
        "def tokenize_doc(text):\n",
        "  tokenized_doc = []\n",
        "  for i in range(len(text)):\n",
        "    # Tokenize sentences - for only doc1\n",
        "    tokenized_sentence = sent_tokenize(text[i])\n",
        "    #print(\"\\ntokenized_sentence: \")\n",
        "    #print(tokenized_sentence)\n",
        "\n",
        "    # Remove punctuations - for only doc1\n",
        "    punc_free_doc1 = re.sub(r'[^\\w\\s]','',text[i])\n",
        "    #print(\"\\npunc_free_sentence: \")\n",
        "    #print(punc_free_doc1)\n",
        "\n",
        "    # Tokenize words - for only doc1\n",
        "    tokenized_doc1 = word_tokenize(punc_free_doc1)\n",
        "    #print(\"\\ntokenized_word: \")\n",
        "    #print(tokenized_doc1)\n",
        "\n",
        "    # Convert the tokens into lowercase: lower_tokens\n",
        "    lower_tokens = [t.lower() for t in tokenized_doc1]\n",
        "    #print(\"\\nlower_case: \")\n",
        "    #print(lower_tokens)\n",
        "\n",
        "    # stop word removal\n",
        "    sww = sw.words()\n",
        "    tokenized_doc1 = [w for w in lower_tokens if not w in sww]\n",
        "    #print(\"\\ntokenized_word (in lower case, w/o stopwords): \")\n",
        "    tokenized_doc.append(tokenized_doc1)\n",
        "  return tokenized_doc\n",
        "\n"
      ],
      "metadata": {
        "id": "0NhoI8FRbRSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "782bc876-1c53-49cb-c7df-efc35f2298d1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing the training set : \n",
        "def exercise4 (tokenized_doc):\n",
        "  model = FastText(tokenized_doc, vector_size=200, window=5, min_count=5, workers=2, sg=1)\n",
        "\n",
        "  \n",
        "  vocab, vectors = model.wv.key_to_index, model.wv.vectors\n",
        "\n",
        "  doc_mean = []\n",
        "  for doc in tokenized_doc:\n",
        "    doc_mat = []\n",
        "    for word in doc :\n",
        "      if word in vocab:\n",
        "        doc_mat.append(model.wv[word])\n",
        "    if len(doc_mat) != 0:\n",
        "      doc_mean.append(np.mean(doc_mat, axis=0))\n",
        "    else:\n",
        "      doc_mean.append(np.zeros(100))\n",
        "\n",
        "   \n",
        "  x = torch.Tensor(doc_mean)\n",
        "  #x = x.mean(dim=1, keepdim=True)\n",
        "  return x\n"
      ],
      "metadata": {
        "id": "6gHJz5By_9U5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the tensor\n",
        "training= tokenize_doc(training_text)\n",
        "w = exercise4(training)"
      ],
      "metadata": {
        "id": "7AV7xvsMGfh4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "l_model = nn.Linear(200, 1)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.SGD(l_model.parameters(), lr=1e-4)\n",
        "\n",
        "# Set the number of epochs\n",
        "num_epochs = 200\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    y_pred = l_model(w)\n",
        "    loss = mse(y_pred, y_data)\n",
        "    #print(\"Optimised:\", \"training loss=\", \"{:.9f}\".format(loss.data))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n"
      ],
      "metadata": {
        "id": "xP4V7fEoHl0D"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Evaluation [1pt]\n",
        "\n",
        "Evaluate the speed and accuracy of the model with your ideas"
      ],
      "metadata": {
        "id": "btzsdyCTrW1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "test = tokenize_doc(test_text)\n",
        "x_test = exercise4(test)\n",
        "y_test = torch.Tensor(test_lengths)\n",
        "testing_loss = mse(l_model(x_test), y_test) \n",
        "print(\"Testing loss=\", \"{:.9f}\".format(testing_loss.data))\n",
        "print(\"Absolute mean square loss difference:\", \"{:.9f}\".format(abs(\n",
        "      training_loss.data - testing_loss.data)))"
      ],
      "metadata": {
        "id": "nwJH-lB5rW-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d40aac-6862-4a15-cb40-18f6e4182a9f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing loss= 0.137733027\n",
            "Absolute mean square loss difference: 0.056478374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **this** text box, briefly describe the results. Did your improvement work? Why / Why not?\n",
        "\n",
        "The improvement worked slightly and better for newer vocabulary since it uses FastText.\n",
        "\n",
        "I found that in this case increasing the vector size doesn't have much of an effect.\n",
        "\n",
        "Otherwise the improvement is very minor and can be ignored. The accuracy is affected by the lack of similarity between the words in training, development and test."
      ],
      "metadata": {
        "id": "xBTz8zwRRLVQ"
      }
    }
  ]
}
